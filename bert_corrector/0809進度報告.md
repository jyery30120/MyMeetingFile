# 0809進度報告_Keras-bert

暫時用Keras-BERT  
原因：Keras-BERT 有明確的load model方法，PyTorch暫時沒找到如何load自己訓練完的model的方法

Pretrain：跑Google Bert( https://github.com/google-research/bert )下面兩個檔案

```shell
python create_pretraining_data.py \
  --input_file=./sample_text.txt \
  --output_file=/tmp/tf_examples.tfrecord \
  --vocab_file=$BERT_BASE_DIR/vocab.txt \
  --do_lower_case=True \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --masked_lm_prob=0.15 \
  --random_seed=12345 \
  --dupe_factor=5
```

上面--input_file=./sample_text.txt \ 換成自己的檔案

```shell
python run_pretraining.py \
  --input_file=/tmp/tf_examples.tfrecord \
  --output_dir=/tmp/pretraining_output \
  --do_train=True \
  --do_eval=True \
  --bert_config_file=$BERT_BASE_DIR/bert_config.json \
  --init_checkpoint=$BERT_BASE_DIR/bert_model.ckpt \
  --train_batch_size=32 \
  --max_seq_length=128 \
  --max_predictions_per_seq=20 \
  --num_train_steps=20 \
  --num_warmup_steps=10 \
  --learning_rate=2e-5
```

上面epoch從20改到500 跑出來的結果

```shell
//init
 ***** Eval results *****
global_step = 20
loss = 2.3418965
masked_lm_accuracy = 0.58585525
masked_lm_loss = 2.0519152

//--num_train_steps=50 \
***** Eval results *****
global_step = 50
loss = 1.863078
masked_lm_accuracy = 0.65703946
masked_lm_loss = 1.6333402

//--num_train_steps=100
***** Eval results *****
global_step = 100
loss = 1.6924309
masked_lm_accuracy = 0.6830921
masked_lm_loss = 1.4934384

//--num_train_steps=200 \
I0806 03:37:43.356990 140515451635584 run_pretraining.py:483] ***** Eval results *****
global_step = 200
loss = 1.5989627
masked_lm_accuracy = 0.69796056
masked_lm_loss = 1.3985518

//--num_train_steps=500 \
***** Eval results *****
global_step = 500
loss = 1.5109024
masked_lm_accuracy = 0.70651317
masked_lm_loss = 1.327778
```

以上是 masked_lm_accuracy 的變化  
然後套用到Keras bert

## keras bert

基本就照著( https://github.com/CyberZHG/keras-bert/blob/master/demo/load_model/keras_bert_load_and_predict.ipynb )打

```shell
import os

pretrained_path = 'chinese_L-12_H-768_A-12'
config_path = os.path.join(pretrained_path, 'bert_config.json')
checkpoint_path = os.path.join(pretrained_path, 'bert_model.ckpt')
vocab_path = os.path.join(pretrained_path, 'vocab.txt')

```
以上跑一次後再把pretrained_path換成自己的Model資料夾再跑一次做比較

然後...可以看到建議的字的部分有改變
但錯字率反而變更高

BERT 的模型：預測錯字數目: 40 錯誤率 0.3669724770642202  
自己訓練的模型：預測錯字數目: 42 錯誤率 0.3853211009174312

列為未來改進方向

繼續努力方向：

1. 拉高epoch 提高 masked_lm_accuracy 但還是要避免overfitting
2. 透過自己的文本去fine-tune好的模型降低在目前文本的抓錯率
3. 針對詞做兩個以上的[MASK] 做錯詞辨識
